---
layout: posts
title:  "zero waste community"
date:   2021-10-25 00:00:00 +0100
categories: stats
entries_layout: grid
---

## Introduction

The "zero waste concept" developed in the United States in the 1970s. A trash disposal company was processing electronic chemicals to increase the reuse rate of things and raw materials while avoiding waste formation, which would result in the waste of a significant quantity of land resources. After being proposed, the concept was initially used in the urban environmental management of Australia's capital, Canberra. Canberra was also the first to establish the objective of creating a "zero waste city" based on the "zero waste" concept. (Zhang, 2021).

## Rules

1. Work Plan for the Construction of" Zero Waste Cities "during the 14th Five Year Plan Period

2. Planning Standards for Urban Environmental Sanitation Facilities (GB/T50337-2018)

This standard includes land use rules for domestic trash transfer facilities, along with a methodology for calculating the production of urban domestic garbage.

**Land Use Rules:**

![](/images/chart1.jpg)

**Basic Equation for the Generation of Household Garbage in Urban Areas:**

The formula for calculating the maximum daily output of residential garbage (Q) is given by:
$$ Q = \frac{R \times C \times A}{1000} $$

- \( Q \) — The maximum daily output of residential garbage, measured in tonnes per day (t/d).
- \( R \) — Denotes the planned population, measured in persons.
- \( C \) — The predicted average daily domestic waste production per capita, which ranges from 0.8 kg/person·day to 1.4 kg/person·day.
- \( A \) — Coefficient of irregular daily output of domestic trash, which may range from 1 to 1.5.
	

## Project Target
![](/images/pic1.png)

## Project Target
### Volumes in High Dimensions

A lot of probability theory uses the idea of volumes. For example, the probability and expectation are usually defined as:

$$ \mathbb P_{\mu} (\mathcal A) = \int_A d \mu \;\;\; ; \;\;\; \mathbb E_{\mu} (f(X)) = \int_{\chi} f(x) d \mu(x) $$

... where the differential element is usually a volume, $$ d \mu(x) = p(x) dx_1 ... dx_d$$, and p is a density, but volumes are really weird in higher dimensions.

The main idea is that our ideas of _distance_ no longer make sense. Consider the following: I simulated a bunch of random normal points in many dimensions and plotted the average ratio of the distance to nearest point to the distance to farthest point:

<center> <img style="max-width: 500px; height: auto;" src = "/images/hidst.png"> </center>

This ratio tends to one, implying that the nearest and farthest points are equally "close" / all points "look alike".

On the same subject, consider a cube of unit lengths containing a sphere of unit radius in higher dimensions:

<center> <img style="max-width: 400px; height: auto;" src = "/images/cubes.png"> </center>
<center> <img style="max-width: 500px; height: auto;" src = "/images/sphre.png"> </center>

The edges of the cube keep more and more volume away from the sphere. In fact - the volume of a sphere starts decreasing after 5 dimensions.

### Typical Sets

Consider the following: In one dimension, if I draw from a normal distribution, numbers close to zero would be the most reasonable guess at what the draw could be. In higher dimensions, this isn't really the case - if I draw from a 1000-dimensional normal, it's ridiculously unlikely that I'd pick a vector that (0, 0, 0, ...); a more reasonable guess would be "a vector of points where the mean radius is ye much".

Now consider the likelihood of an isolated repeated experiment:

$$L(\theta | x) = \prod_i^n p(x_i|\theta)$$

This is a function of many random variables (which collectively have a dimension n). This will have it's own sampling distribution, and the position where it's maxmized w.r.t. any of the x's isn't a typical value of the likelihood at all.

Example:

{%highlight ruby%}

hist_fancy(replicate(10000, sum(log(dnorm(rnorm(100))))),
           main = "Typical Values of the Loglikelihood", xlab = "ll")

{% endhighlight %}

<center> <img style="max-width: 500px; height: auto;" src = "/images/typlk.png"> </center>

The maximum possible value of the loglikelihood in this case is -91.9, although it's always much lower than that. Loglikelihood values around -140 _for this experiment_ are _typical_.

We call values that lie in this region of typicality the ***typical set*** and it's geometry is expected to be weird. From the other results, we expect it to be a thin layer of mass lying on a shell somewhere in a high dimensional space. The booklet "Logic, Probability, and Bayesian Inference" by Betancourt & the Stan Dev Team has another such example.

From Andrew Gelman's blog: Interpolating between any two values of the typical set is rather unlikely to result in another typical sample. To sample from an arbitrary density, we can't use grid or ensemble based methods, so a logical alternative is to start with a sample and obtain another sample using it, which leads logically to the idea of Markov Chain Monte Carlo, at which the Hybrid/Hamiltonian method is rather efficient.

<details>
<summary> R Code for Dimensionality Image </summary>

{%highlight ruby%}

library(mvtnorm); library(ggplot2)

dim_dist <- function(d, samp_size = 10, ...){
	samp <- rmvnorm(samp_size, mean = rep(0, d), sigma = diag(1, d, d))
	dists <- as.matrix(dist(samp, diag = T, upper = T, ...))
	return(mean(sapply(1:samp_size, function(i) min(dists[-i, i])/max(dists[-i, i]))))
}

qplot(x = 1:250,
	  y = sapply(1:250, function(d) dim_dist(d)),
	  main = "Euclidean Distance in High Dimensions",
	  ylab = "Average Ratio of Nearest to Furthest Point",
	  xlab = "Dimension", color = I("dark gray"), ylim = c(0, 1)) +
	  geom_hline(aes(yintercept = 1))

{% endhighlight %}

</details> <br>
